{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashish-AVS/Machine-Learning-Codes/blob/main/ML_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c35fb3f2",
      "metadata": {
        "scrolled": false,
        "id": "c35fb3f2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "metadata": {
        "id": "0t7X0fR1gTf_"
      },
      "id": "0t7X0fR1gTf_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n"
      ],
      "metadata": {
        "id": "KuOOG_Eugbmt"
      },
      "id": "KuOOG_Eugbmt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://drive.google.com/file/d/1zy8g9zC6qSu7fTFABtfdcYUyKBGZ24xq/view"
      ],
      "metadata": {
        "id": "7x89zYxog6iU"
      },
      "id": "7x89zYxog6iU"
    },
    {
      "cell_type": "code",
      "source": [
        "fileDownloaded = drive.CreateFile({'id':'1zy8g9zC6qSu7fTFABtfdcYUyKBGZ24xq'})\n",
        "fileDownloaded.GetContentFile('ML_dataset.csv')\n",
        "df = pd.read_csv('ML_dataset.csv')\n",
        "display(df)"
      ],
      "metadata": {
        "id": "8S8JREVmg636"
      },
      "id": "8S8JREVmg636",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature Engineering Task 1**"
      ],
      "metadata": {
        "id": "mGd2uOCVO0EG"
      },
      "id": "mGd2uOCVO0EG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "890d8578",
      "metadata": {
        "id": "890d8578"
      },
      "outputs": [],
      "source": [
        "cols = df.columns\n",
        "num_cols = df._get_numeric_data().columns[1:]\n",
        "#identifying which column contains continuous numeric data\n",
        "num_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d31a4f6",
      "metadata": {
        "id": "2d31a4f6"
      },
      "outputs": [],
      "source": [
        "categorical_cols = list(set(cols)-set(num_cols))\n",
        "categorical_cols\n",
        "#columns with categorical data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "352b24b0",
      "metadata": {
        "scrolled": true,
        "id": "352b24b0"
      },
      "outputs": [],
      "source": [
        "#finding missing total missing values in each column\n",
        "missing_values = sum(list(df.isna().sum()))\n",
        "#print(missing_values)\n",
        "#there were 7 missing values initially"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ace76ce",
      "metadata": {
        "id": "1ace76ce"
      },
      "outputs": [],
      "source": [
        "for col in df:\n",
        "    if col not in ['id','diagnosis']:\n",
        "        mean_value = df[col].mean()\n",
        "        #calculating the mean value\n",
        "        df[col].fillna(value=mean_value,inplace=True)\n",
        "        #replacing empty cell with the mean value calculated as mentioned in task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature Engineering Task 2**"
      ],
      "metadata": {
        "id": "PkCKvyw_KkgJ"
      },
      "id": "PkCKvyw_KkgJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfdb2565",
      "metadata": {
        "id": "dfdb2565"
      },
      "outputs": [],
      "source": [
        "df_norm = df\n",
        "#here I created a dummy dataframe and normalized it as mentioned in task 2\n",
        "for col in df_norm:\n",
        "    if col not in ['id','diagnosis']:\n",
        "        df_norm[col] = (df_norm[col] - df_norm[col].mean()) / df_norm[col].std()\n",
        "df_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6989fb7a",
      "metadata": {
        "id": "6989fb7a"
      },
      "outputs": [],
      "source": [
        "missing_value1 = df['diagnosis'].isna().sum()\n",
        "missing_value2 = df['id'].isna().sum()\n",
        "#finding missing value in our categorical data\n",
        "#here there was no missing value in both\n",
        "print(missing_value1,missing_value2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['diagnosis'].value_counts()"
      ],
      "metadata": {
        "id": "RlYjtK_vzSFk"
      },
      "id": "RlYjtK_vzSFk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Perceptron Learning Algorithm**"
      ],
      "metadata": {
        "id": "cXahyuMQPZsw"
      },
      "id": "cXahyuMQPZsw"
    },
    {
      "cell_type": "code",
      "source": [
        "#task 2 was accomplished\n",
        "\n",
        "class Perceptron:\n",
        "    \n",
        "    def __init__(self, learning_rate = 0.001, epochs = 1000):\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def activation(self, z):\n",
        "        return np.heaviside(z, 0)\n",
        "    \n",
        "#                       0   if x1 < 0\n",
        "# heaviside(x1, x2) =  x2   if x1 == 0\n",
        "# here x2 = 0\n",
        "#                       1   if x1 > 0\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        columns = X.shape[1]\n",
        "        # This will give the number of columns is 2-d array \n",
        "        # Initializing weights and bias with zeros\n",
        "        self.weights = np.zeros((columns))\n",
        "        self.bias = 0\n",
        "        \n",
        "        # Iterating until the number of epochs\n",
        "        for epoch in range(self.epochs):\n",
        "            \n",
        "            # Traversing through the entire training set\n",
        "            for i in range(len(X)):\n",
        "                z = np.dot(X, self.weights) + self.bias # Finding the dot product and adding the bias\n",
        "                y_pred = self.activation(z) # Passing through an activation function\n",
        "                \n",
        "                #Updating weights and bias\n",
        "                self.weights = self.weights + self.learning_rate * (y[i] - y_pred[i]) * X[i]\n",
        "                self.bias = self.bias + self.learning_rate * (y[i] - y_pred[i])\n",
        "                \n",
        "        return self.weights, self.bias\n",
        "    \n",
        "    def predict(self, X):\n",
        "        z = np.dot(X, self.weights) + self.bias\n",
        "        return self.activation(z)"
      ],
      "metadata": {
        "id": "tPi25n07PW2Y"
      },
      "id": "tPi25n07PW2Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PM1**"
      ],
      "metadata": {
        "id": "0nrxz7jCO-sT"
      },
      "id": "0nrxz7jCO-sT"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X = df.iloc[:, 2:].values\n",
        "y = np.where(df.iloc[:, 1].values == 'M', 1, 0)\n",
        "  #create a numpy array and put value where the cancer is malignant to 1 if not then 0\n",
        "  #above function is a compact form of the below function\n",
        "\n",
        "  # li = []\n",
        "  # for i in df.iloc[:,1].values:\n",
        "  #     if i == 'M':\n",
        "  #         li.append(1)\n",
        "  #     else:\n",
        "  #         li.append(-1)\n",
        "  # y = np.array(li)\n",
        "\n",
        "  # set a random seed for reproducibility\n",
        "np.random.seed(1)\n",
        "  # shuffle the indices of the samples\n",
        "indices = np.arange(len(X))\n",
        "  #The np.arange() function is used to create an array of indices \n",
        "      #corresponding to the number of samples in the dataset\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "  # split the indices into training and testing sets as ***mentioned by Bhanu Sir***\n",
        "n_train = int(len(X) * 2/3)\n",
        "train_indices, test_indices = indices[:n_train], indices[n_train:]\n",
        "\n",
        "  # create the training and testing feature and target arrays\n",
        "X_train, y_train = X[train_indices], y[train_indices]\n",
        "X_test, y_test = X[test_indices], y[test_indices]\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "PrswV6LhGAsf"
      },
      "id": "PrswV6LhGAsf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "043724a4",
      "metadata": {
        "id": "043724a4"
      },
      "outputs": [],
      "source": [
        "perceptron = Perceptron()\n",
        "\n",
        "perceptron.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing dataset\n",
        "y_pred = perceptron.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model on the testing dataset\n",
        "accuracy = np.sum(y_pred == y_test)/len(y_test) * 100\n",
        "print(f\"Accuracy: {accuracy} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PM2**"
      ],
      "metadata": {
        "id": "esmjREX4Pq-o"
      },
      "id": "esmjREX4Pq-o"
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.iloc[:, 2:].values\n",
        "# Get Malignant cancer datapoints and label them as 1, and begnign as 0\n",
        "y = np.where(df.iloc[:, 1].values == 'M', 1, 0)\n",
        "  #create a numpy array and put value where the cancer is malignant to 1 if not then 0\n",
        "  #above function is a compact form of the below function\n",
        "\n",
        "  # li = []\n",
        "  # for i in df.iloc[:,1].values:\n",
        "  #     if i == 'M':\n",
        "  #         li.append(1)\n",
        "  #     else:\n",
        "  #         li.append(-1)\n",
        "  # y = np.array(li)\n",
        "\n",
        "  # set a random seed for reproducibility\n",
        "np.random.seed(1)\n",
        "  # shuffle the indices of the samples\n",
        "indices = np.arange(len(X))\n",
        "  #The np.arange() function is used to create an array of indices \n",
        "      #corresponding to the number of samples in the dataset\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "  # split the indices into training and testing sets as ***mentioned by Bhanu Sir***\n",
        "n_train = int(len(X) * 2/3)\n",
        "train_indices, test_indices = indices[:n_train], indices[n_train:]\n",
        "\n",
        "# Shuffle the train_indices\n",
        "np.random.shuffle(train_indices)\n",
        "X_train, y_train = X[train_indices], y[train_indices]\n",
        "\n",
        "X_test, y_test = X[test_indices], y[test_indices]"
      ],
      "metadata": {
        "id": "-rLSHvl_PyHI"
      },
      "id": "-rLSHvl_PyHI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perceptron = Perceptron()\n",
        "perceptron.fit(X_train, y_train)\n",
        "y_pred = perceptron.predict(X_test)\n",
        "accuracy = np.sum(y_pred == y_test)/len(y_test) * 100\n",
        "print(f\"Accuracy: {accuracy} %\")\n",
        "\n",
        "#accuracy of PM2 matches with accuracy of PM1"
      ],
      "metadata": {
        "id": "QD3qCao1QQKK"
      },
      "id": "QD3qCao1QQKK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PM3**"
      ],
      "metadata": {
        "id": "kfvo3wcfSFoC"
      },
      "id": "kfvo3wcfSFoC"
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_norm.iloc[:, 2:].values\n",
        "y = np.where(df_norm.iloc[:, 1].values == 'M', 1, 0)\n",
        "\n",
        "\n",
        "np.random.seed(1)\n",
        "indices = np.arange(len(X))\n",
        "np.random.shuffle(indices)\n",
        "n_train = int(len(X) * 2/3)\n",
        "train_indices, test_indices = indices[:n_train], indices[n_train:]\n",
        "X_train, y_train = X[train_indices], y[train_indices]\n",
        "X_test, y_test = X[test_indices], y[test_indices]\n",
        "perceptron = Perceptron()\n",
        "perceptron.fit(X_train, y_train)\n",
        "y_pred = perceptron.predict(X_test)\n",
        "accuracy = np.sum(y_pred == y_test)/len(y_test) * 100\n",
        "print(f\"Accuracy: {accuracy} %\")\n"
      ],
      "metadata": {
        "id": "yGCf5t5DSOE6"
      },
      "id": "yGCf5t5DSOE6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Set number of folds\n",
        "# k = 5\n",
        "\n",
        "# # Shuffle data and split into k folds\n",
        "# n_samples = X.shape[0]\n",
        "# fold_size = n_samples // k\n",
        "# indices = np.random.permutation(n_samples)\n",
        "# X_shuffled, y_shuffled = X[indices], y[indices]\n",
        "# X_folds = [X_shuffled[i:i+fold_size] for i in range(0, n_samples, fold_size)]\n",
        "# y_folds = [y_shuffled[i:i+fold_size] for i in range(0, n_samples, fold_size)]\n",
        "\n",
        "# # Train and test model on each fold\n",
        "# accuracies = []\n",
        "# for i in range(k):\n",
        "#     # Get training and test data\n",
        "#     X_test_fold = X_folds[i]\n",
        "#     y_test_fold = y_folds[i]\n",
        "#     X_train_folds = X_folds[:i] + X_folds[i+1:]\n",
        "#     y_train_folds = y_folds[:i] + y_folds[i+1:]\n",
        "#     X_train = np.concatenate(X_train_folds)\n",
        "#     y_train = np.concatenate(y_train_folds)\n",
        "\n",
        "#     # Fit perceptron to training data\n",
        "#     perceptron = Perceptron()\n",
        "#     perceptron.fit(X_train, y_train)\n",
        "\n",
        "#     # Test perceptron on test data and compute accuracy\n",
        "#     y_pred = perceptron.predict(X_test_fold)\n",
        "#     accuracy = np.mean(y_pred == y_test_fold)\n",
        "#     accuracies.append(accuracy)\n",
        "\n",
        "# # Compute average accuracy and variance\n",
        "# mean_accuracy = np.mean(accuracies)\n",
        "# variance = np.var(accuracies)\n",
        "\n",
        "# print(\"Average accuracy:\", mean_accuracy)\n",
        "# print(\"Variance in accuracy:\", variance) "
      ],
      "metadata": {
        "id": "RpuaFIgwVDEw"
      },
      "id": "RpuaFIgwVDEw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PM4**"
      ],
      "metadata": {
        "id": "ogJ7KPXq4vSW"
      },
      "id": "ogJ7KPXq4vSW"
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_cols = df.iloc[:, 2:].sample(frac=1, axis=1)\n",
        "df2 = pd.concat([df.iloc[:, :2], shuffled_cols], axis=1)\n",
        "X = df2.iloc[:, 2:].values\n",
        "y = np.where(df2.iloc[:, 1].values == 'M', 1, 0)\n",
        "np.random.seed(1)\n",
        "indices = np.arange(len(X))\n",
        "np.random.shuffle(indices)\n",
        "n_train = int(len(X) * 2/3)\n",
        "train_indices, test_indices = indices[:n_train], indices[n_train:]\n",
        "X_train, y_train = X[train_indices], y[train_indices]\n",
        "X_test, y_test = X[test_indices], y[test_indices]"
      ],
      "metadata": {
        "id": "qsrqhKPz6Rlr"
      },
      "id": "qsrqhKPz6Rlr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perceptron = Perceptron()\n",
        "perceptron.fit(X_train, y_train)\n",
        "y_pred = perceptron.predict(X_test)\n",
        "accuracy = np.sum(y_pred == y_test)/len(y_test) * 100\n",
        "print(f\"Accuracy: {accuracy} %\")\n",
        "\n",
        "#accuracy of PM2 matches with accuracy of PM1"
      ],
      "metadata": {
        "id": "NJ04iPsxDeCB"
      },
      "id": "NJ04iPsxDeCB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Fisher’s Linear Discriminant Analysis**\n"
      ],
      "metadata": {
        "id": "mlMeeu186TuP"
      },
      "id": "mlMeeu186TuP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy.linalg import inv as inv\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm as norm\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "ntuXl2Zc6WU0"
      },
      "id": "ntuXl2Zc6WU0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_predictions(X_test, W, threshold):\n",
        "    \n",
        "    # Project the Test data onto W\n",
        "    proj_testdata = np.dot(W, X_test.transpose()).reshape(X_test.shape[0],1)\n",
        "    \n",
        "    # Compare with threshold\n",
        "    predictions = (proj_testdata >= threshold).astype(int).reshape(X_test.shape[0],1)\n",
        "    \n",
        "    return proj_testdata, predictions"
      ],
      "metadata": {
        "id": "z8X4kdFf60qb"
      },
      "id": "z8X4kdFf60qb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_projections(projections, y_test, threshold):\n",
        "    proj_pred_data = pd.DataFrame(np.concatenate((projections, y_test.reshape(-1, 1)), axis=1))\n",
        "    \n",
        "\n",
        "    proj1_pred = proj_pred_data.loc[proj_pred_data[1] == 1][[0]]\n",
        "    proj2_pred = proj_pred_data.loc[proj_pred_data[1] == 0][[0]]    \n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(proj1_pred, np.ones(proj1_pred.shape), '.', color='b', label='Malignant (value 1)')\n",
        "    plt.plot(proj2_pred, np.ones(proj2_pred.shape), '.', color='r',label='Benign (value 0)')\n",
        "    plt.plot([threshold], [1], '.', color='g',label='Threshold')\n",
        "    plt.legend(loc = 'upper right')\n",
        "    plt.title('Projections onto W vector. Threshold = '+ str(threshold))\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Jagpx-xN83p_"
      },
      "id": "Jagpx-xN83p_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_w_cordinates(X_train1, X_train2):\n",
        "    # X_train1 is the positive class and X_train2 is the negative class\n",
        "    \n",
        "    # 1. Find M1, M2, Cov1, Cov2\n",
        "    Mean1 = np.mean(X_train1,axis=0)\n",
        "    Mean2 = np.mean(X_train2,axis=0)\n",
        "    Mean_difference = np.subtract(Mean1,Mean2)\n",
        "\n",
        "    Cov1 = np.cov(np.transpose(X_train1))\n",
        "    Cov2 = np.cov(np.transpose(X_train2))\n",
        "\n",
        "    # 2. Within class spread Sw = Cov1 + Cov2  \n",
        "    Sw_inv = np.linalg.inv(np.add(Cov1,Cov2))\n",
        "    \n",
        "    # 3. W = (Sw_inv).(M1-M2)\n",
        "    W = np.dot(Sw_inv,Mean_difference)\n",
        "    \n",
        "    # 4. Normalise W\n",
        "    W_norm = W / np.linalg.norm(W)\n",
        "    \n",
        "    # Note:\n",
        "    # change X limits for pdf plot to -30 to 30 if W is used\n",
        "    # change X limits for pdf plot to -3 to 3 if normalised W is used\n",
        "    \n",
        "    return W_norm"
      ],
      "metadata": {
        "id": "8JBxuR8_6YFj"
      },
      "id": "8JBxuR8_6YFj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def solve_pdfs(pdf_mean_1, pdf_mean_2, pdf_std_1, pdf_std_2):\n",
        "    coeff1 = 1/(2*pdf_std_1**2) - 1/(2*pdf_std_2**2)\n",
        "    coeff2 = pdf_mean_2/(pdf_std_2**2) - pdf_mean_1/(pdf_std_1**2)\n",
        "    coeff3 = pdf_mean_1**2 /(2*pdf_std_1**2) - pdf_mean_2**2 / (2*pdf_std_2**2) - np.log(pdf_std_2/pdf_std_1)\n",
        "\n",
        "    coeffs = [coeff1, coeff2, coeff3]\n",
        "    roots_of_eq = np.roots(coeffs)\n",
        "    threshold = roots_of_eq[1]\n",
        "    #found by experimenting\n",
        "    #take[1] when using normalized value\n",
        "    #else take[0] as [1] gives negative solution or the other incorrect solution\n",
        "    return threshold"
      ],
      "metadata": {
        "id": "FD7LYrDn6w5h"
      },
      "id": "FD7LYrDn6w5h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_pdfs(mean1, mean2, std1, std2, threshold):\n",
        "    # change X limits for pdf plot to -3 to 3 if normalised W is used. Otherwise set it to -30 to 30 \n",
        "    X_axis = np.linspace(-1,1,100000)\n",
        "    plt.plot(X_axis, norm.pdf(X_axis, mean1, std1),'-', color='b',label='Malignant (value 1)')    \n",
        "    plt.plot(X_axis, norm.pdf(X_axis, mean2, std2),'-', color='r',label='Benign (value 0)')\n",
        "    plt.plot([threshold], norm.pdf([threshold], mean1, std1), '*', color='g',label='Threshold')\n",
        "    plt.legend(loc = 'upper right')\n",
        "    plt.title('Normal distributions. Threshold = '+ str(threshold))\n",
        "    plt.show()\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "9ZnBW_iW7Ses"
      },
      "id": "9ZnBW_iW7Ses",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def find_threshold(X_train1, X_train2, W):\n",
        "    # Threshold = point of intersection of the (probability density functions)\n",
        "    \n",
        "    # 1. Find Projections on the line W\n",
        "    Proj1 = np.dot(W,X_train1.transpose())\n",
        "    Proj2 = np.dot(W,X_train2.transpose())\n",
        "    \n",
        "    # 2. Find Means of the projected data for positive and negative classes\n",
        "    proj_mean1 = np.mean(Proj1)\n",
        "    proj_mean2 = np.mean(Proj2)\n",
        "    print('malignant_mean',proj_mean1,' benign_mean2',proj_mean2)\n",
        "    # 3. Find Std deviations of the projected data for positive and negative classes\n",
        "    proj_std1 = np.std(Proj1)\n",
        "    proj_std2 = np.std(Proj2)\n",
        "    print('malignant_std',proj_std1,' benign_std2',proj_std2)\n",
        "    # 4. Solve the equations\n",
        "\n",
        "    threshold = solve_pdfs(proj_mean1, proj_mean2, proj_std1, proj_std2)\n",
        "    print(\"threshold\",threshold)\n",
        "    print('\\n')\n",
        "    # Plot the pdfs\n",
        "    plot_pdfs(proj_mean1, proj_mean2, proj_std1, proj_std2, threshold)\n",
        "    return threshold\n"
      ],
      "metadata": {
        "id": "nEJ0pfAh7H4P"
      },
      "id": "nEJ0pfAh7H4P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "vr4DGeMcdHzS"
      },
      "id": "vr4DGeMcdHzS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**“TP” = true positive; “FN” = false negative; “FP” = false positive; “TN” = true negative.**"
      ],
      "metadata": {
        "id": "wVzZ2XV0dopD"
      },
      "id": "wVzZ2XV0dopD"
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(actual, predicted):\n",
        "    # Accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
        "    return np.mean((actual == predicted).astype(int))"
      ],
      "metadata": {
        "id": "8eSG0Ey77Y2k"
      },
      "id": "8eSG0Ey77Y2k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precision(actual, predicted):\n",
        "    # Precision = (TP)/(TP + FP) = True pos out of the actual results\n",
        "    \n",
        "    TP = np.count_nonzero(np.logical_and((actual == 1) , (predicted == 1)).astype(int))\n",
        "    FP = np.count_nonzero(np.logical_and((actual == 0) , (predicted == 1)).astype(int))\n",
        "    return TP/(TP + FP)"
      ],
      "metadata": {
        "id": "HFf12S6VcYaX"
      },
      "id": "HFf12S6VcYaX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recall(actual, predicted):\n",
        "    # Recall = (TP)/(TP + FN) = True pos out of the predicted results\n",
        "    \n",
        "    TP = np.count_nonzero(np.logical_and((actual == 1) , (predicted == 1)).astype(int))\n",
        "    FN = np.count_nonzero(np.logical_and((actual == 1) , (predicted == 0)).astype(int))\n",
        "    return TP/(TP + FN)"
      ],
      "metadata": {
        "id": "BLEjePH8cch9"
      },
      "id": "BLEjePH8cch9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f_score(actual , predicted):\n",
        "    # F_Score = 2PR/(P+R)\n",
        "    \n",
        "    P = precision(actual, predicted)\n",
        "    R = recall(actual, predicted)\n",
        "    return 2*P*R/(P + R)"
      ],
      "metadata": {
        "id": "Oou5HKeVclMn"
      },
      "id": "Oou5HKeVclMn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fisher(X_train1, X_train2, X_test, y_test):\n",
        "    \n",
        "    # 1. Find Projection Line\n",
        "    W = find_w_cordinates(X_train1, X_train2)\n",
        "    \n",
        "    # 2. Find Threshold\n",
        "    threshold = find_threshold(X_train1, X_train2, W)\n",
        "    \n",
        "    # 3. Find Predictions for the Test Data by Projecting onto W and comparing to the threshold\n",
        "    projections, y_predicted = find_predictions(X_test, W, threshold)\n",
        "    \n",
        "    # Plot the projections\n",
        "    plot_projections(projections, y_test, threshold)\n",
        "    print('\\n')\n",
        "    print('W:\\n',W)\n",
        "    print('\\n')\n",
        "    print('Threshold:',threshold)\n",
        "    print('accuracy:',accuracy(y_test , y_predicted))\n",
        "    \n",
        "    print(\"precision:\",precision(y_test , y_predicted))\n",
        "    print(\"recall:\",recall(y_test , y_predicted))\n",
        "    print(\"f score:\",f_score(y_test , y_predicted))"
      ],
      "metadata": {
        "id": "KHDVtT097bHH"
      },
      "id": "KHDVtT097bHH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FLDM1**"
      ],
      "metadata": {
        "id": "Jh5w1tOBt6s8"
      },
      "id": "Jh5w1tOBt6s8"
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.iloc[:, 2:].values\n",
        "y = np.where(df.iloc[:, 1].values == 'M', 1, 0)\n",
        "\n",
        "  #create a numpy array and put value where the cancer is malignant to 1 if not then 0\n",
        "  #above function is a compact form of the below function\n",
        "\n",
        "  # li = []\n",
        "  # for i in df.iloc[:,1].values:\n",
        "  #     if i == 'M':\n",
        "  #         li.append(1)\n",
        "  #     else:\n",
        "  #         li.append(-1)\n",
        "  # y = np.array(li)\n",
        "\n",
        "  # set a random seed for reproducibility\n",
        "np.random.seed(13123)\n",
        "  # shuffle the indices of the samples\n",
        "indices = np.arange(len(X))\n",
        "  #The np.arange() function is used to create an array of indices \n",
        "      #corresponding to the number of samples in the dataset\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "  # split the indices into training and testing sets as ***mentioned by Bhanu Sir***\n",
        "n_train = int(len(X) * 2/3)\n",
        "train_indices, test_indices = indices[:n_train], indices[n_train:]\n",
        "\n",
        "  # create the training and testing feature and target arrays\n",
        "X_train, y_train = X[train_indices], y[train_indices]\n",
        "X_test, y_test = X[test_indices], y[test_indices]\n",
        "\n",
        "X_train1 = X_train[y_train == 1]\n",
        "X_train2 =  X_train[y_train == 0]"
      ],
      "metadata": {
        "id": "CDEUNIY0t42u"
      },
      "id": "CDEUNIY0t42u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fisher(X_train1, X_train2, X_test, y_test)"
      ],
      "metadata": {
        "id": "ea30qGmp-0WS"
      },
      "id": "ea30qGmp-0WS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FLDM2**"
      ],
      "metadata": {
        "id": "VBYF7cKBAyRJ"
      },
      "id": "VBYF7cKBAyRJ"
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_cols = df.iloc[:, 2:].sample(frac=1, axis=1)\n",
        "df2 = pd.concat([df.iloc[:, :2], shuffled_cols], axis=1)\n",
        "X = df2.iloc[:, 2:].values\n",
        "y = np.where(df2.iloc[:, 1].values == 'M', 1, 0)\n",
        "np.random.seed(13123)\n",
        "indices = np.arange(len(X))\n",
        "np.random.shuffle(indices)\n",
        "n_train = int(len(X) * 2/3)\n",
        "train_indices, test_indices = indices[:n_train], indices[n_train:]\n",
        "X_train, y_train = X[train_indices], y[train_indices]\n",
        "X_test, y_test = X[test_indices], y[test_indices]\n",
        "\n",
        "X_train1 = X_train[y_train == 1]\n",
        "X_train2 =  X_train[y_train == 0]"
      ],
      "metadata": {
        "id": "dRpLYzirA2eL"
      },
      "id": "dRpLYzirA2eL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fisher(X_train1, X_train2, X_test, y_test)"
      ],
      "metadata": {
        "id": "4tjyRRvdBNcc"
      },
      "id": "4tjyRRvdBNcc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Logistic Regression**"
      ],
      "metadata": {
        "id": "Pb4FUhwF9YnA"
      },
      "id": "Pb4FUhwF9YnA"
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "cHiiK7g-9au9"
      },
      "id": "cHiiK7g-9au9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_gradient_descent(X,y,theta,learning_rate,iterations):\n",
        "    cost_history=np.zeros(iterations)\n",
        "    m=len(y)\n",
        "    for i in range(iterations):\n",
        "        gradient = (1/m)* np.dot(X.transpose(), (hypothesis(X,theta)- y))\n",
        "        #w=w-ita*gradient\n",
        "        theta = theta - learning_rate*gradient\n",
        "        cost_history[i] = cost(X, y, theta)\n",
        "        \n",
        "    return theta,cost_history"
      ],
      "metadata": {
        "id": "kLoTbOMHxCjJ"
      },
      "id": "kLoTbOMHxCjJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to predict the class of a test data point\n",
        "def predict(X, theta, threshold=0.5):\n",
        "    return (hypothesis(X, theta) >= threshold).astype(int)"
      ],
      "metadata": {
        "id": "px7zVGWl9cPn"
      },
      "id": "px7zVGWl9cPn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "Qx4pUse7d6a0"
      },
      "id": "Qx4pUse7d6a0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**“TP” = true positive; “FN” = false negative; “FP” = false positive; “TN” = true negative.**"
      ],
      "metadata": {
        "id": "QIpiANE2eCF2"
      },
      "id": "QIpiANE2eCF2"
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(actual,predicted):\n",
        "     return np.mean((actual == predicted).astype(int))"
      ],
      "metadata": {
        "id": "n2v1T-xK3Su1"
      },
      "id": "n2v1T-xK3Su1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recall(actual, predicted):\n",
        "    TP = np.count_nonzero(np.logical_and((actual == 1) , (predicted ==1)).astype(int))\n",
        "    FN = np.count_nonzero(np.logical_and((actual == 1) , (predicted ==0)).astype(int))\n",
        "    return TP/(TP + FN)"
      ],
      "metadata": {
        "id": "evGHBn8V3Xjt"
      },
      "id": "evGHBn8V3Xjt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f_score(actual , predicted):\n",
        "    P = precision(actual, predicted)\n",
        "    R = recall(actual, predicted)\n",
        "    return 2*P*R/(P + R)"
      ],
      "metadata": {
        "id": "zqsohMhr3eJp"
      },
      "id": "zqsohMhr3eJp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to calculate precision of the classifier\n",
        "def precision(actual, predicted):\n",
        "    TP = np.count_nonzero(np.logical_and((actual == 1) , (predicted ==1)).astype(int))\n",
        "    FP = np.count_nonzero(np.logical_and((actual == 0) , (predicted ==1)).astype(int))\n",
        "    return TP/(TP + FP)"
      ],
      "metadata": {
        "id": "ChwEQ7wo9d5u"
      },
      "id": "ChwEQ7wo9d5u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to calculate sigmoid\n",
        "def sigmoid(x):\n",
        "    return 1/(1 + np.exp(-x) )"
      ],
      "metadata": {
        "id": "GHPsTUHD9hS9"
      },
      "id": "GHPsTUHD9hS9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hypothesis for logistic regression\n",
        "def hypothesis(X,theta):\n",
        "    return sigmoid(np.dot(X, theta))"
      ],
      "metadata": {
        "id": "9T2FnN2q9m1q"
      },
      "id": "9T2FnN2q9m1q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#binary cross entropy cost function, i.e derivative which we have to minimize\n",
        "def cost(X,y,theta):\n",
        "    h = hypothesis(X,theta)\n",
        "    return (-1/len(y))*(np.sum((y * np.log(h)) + (1-y)*np.log(1-h)))"
      ],
      "metadata": {
        "id": "-ef218Ip-I2u"
      },
      "id": "-ef218Ip-I2u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LR1**"
      ],
      "metadata": {
        "id": "BczLL55KgSRa"
      },
      "id": "BczLL55KgSRa"
    },
    {
      "cell_type": "code",
      "source": [
        "# This snippet splits data into test, train and gives out theta"
      ],
      "metadata": {
        "id": "ery9_qRGFzwb"
      },
      "id": "ery9_qRGFzwb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.iloc[:, 2:].values\n",
        "y = np.where(df.iloc[:, 1].values == 'M', 1, 0)\n",
        "\n",
        "#concatenate a columns of ones (to accomodate the bias)\n",
        "ones=np.ones((X.shape[0],1))\n",
        "X=np.concatenate((ones,X),axis=1) \n",
        "\n",
        "#gaussian weight initialisations can be tried:\n",
        "theta=np.random.randn(X.shape[1],1)\n",
        "\n",
        "#split the data into training and testing data, change the test split here\n",
        "np.random.seed(1)\n",
        "indices = np.arange(len(X))\n",
        "np.random.shuffle(indices)\n",
        "# split the indices into training and testing sets as ***mentioned by Bhanu Sir***\n",
        "n_train = int(len(X) * 2/3)\n",
        "train_indices, test_indices = indices[:n_train], indices[n_train:]\n",
        "\n",
        "# create the training and testing feature and target arrays\n",
        "X_train, y_train = X[train_indices], y[train_indices]\n",
        "X_test, y_test = X[test_indices], y[test_indices]\n"
      ],
      "metadata": {
        "id": "b7zpEPhq-bkH"
      },
      "id": "b7zpEPhq-bkH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predicted = predict(X_test,theta) #we put inside it the respective threshold needed\n",
        "print(\"accuracy:\" , accuracy(y_test , y_predicted))\n",
        "print(\"precision:\",precision(y_test , y_predicted))\n",
        "print(\"recall:\",recall(y_test , y_predicted))\n",
        "print(\"f score:\",f_score(y_test , y_predicted))"
      ],
      "metadata": {
        "id": "kbUnUaROUiQZ"
      },
      "id": "kbUnUaROUiQZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tn6ibZt-gkXa"
      },
      "id": "tn6ibZt-gkXa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q) What happens to testing accuracy when you vary the decision probability threshold from 0.5 to 0.3, 0.4, 0.6 and 0.7.\n",
        "\n",
        "## REDUNDANT"
      ],
      "metadata": {
        "id": "ivFNrsO4gZ26"
      },
      "id": "ivFNrsO4gZ26"
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_logistic(threshold=0.3):\n",
        "  y_predicted = predict(X_test,theta,threshold) #we put inside it the respective threshold needed\n",
        "  print(\"accuracy:\" , accuracy(y_test , y_predicted))\n",
        "  print(\"precision:\",precision(y_test , y_predicted))\n",
        "  print(\"recall:\",recall(y_test , y_predicted))\n",
        "  print(\"f score:\",f_score(y_test , y_predicted))"
      ],
      "metadata": {
        "id": "bS7SqnXQglWl"
      },
      "id": "bS7SqnXQglWl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_logistic(0.3)"
      ],
      "metadata": {
        "id": "gfuz9_lFgsh0"
      },
      "id": "gfuz9_lFgsh0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_logistic(0.4)"
      ],
      "metadata": {
        "id": "gxiIi7n1g80o"
      },
      "id": "gxiIi7n1g80o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_logistic(0.5)"
      ],
      "metadata": {
        "id": "mFSTgzLFg1lT"
      },
      "id": "mFSTgzLFg1lT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_logistic(0.6)"
      ],
      "metadata": {
        "id": "8fKMKGcLmbX5"
      },
      "id": "8fKMKGcLmbX5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_logistic(0.7)"
      ],
      "metadata": {
        "id": "THh2NSammcvu"
      },
      "id": "THh2NSammcvu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You must implement the above two learning tasks using Batch Gradient Descent, Mini-batch Gradient Descent and Stochastic Gradient Descent using learning rates 0.01, 0.001 and 0.0001. Plot the learning curves (Cost Function vs Iterations) for the same."
      ],
      "metadata": {
        "id": "ILOE_09T5GOy"
      },
      "id": "ILOE_09T5GOy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Batch Gradient Descent**"
      ],
      "metadata": {
        "id": "cHH--FP5WXzL"
      },
      "id": "cHH--FP5WXzL"
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_gradient_descent(learning_rate=0.01, iterations=1500):\n",
        "  #Batch Gradient Descent\n",
        "  theta,cost_history = batch_gradient_descent(X_train,y_train,theta,learning_rate,iterations)\n",
        "  prediction = predict(X, theta, threshold=0.5)\n",
        "  print(prediction)\n",
        "  plt.title('Learning curve (Cost Function vs Iterations), Weights = normal, LR = {} , itr = {}'.format(learning_rate, iterations))\n",
        "  plt.plot(list(range(iterations)), cost_history, '-r') #plot the cost function.\n",
        "\n"
      ],
      "metadata": {
        "id": "QFLnBu885Wd1"
      },
      "id": "QFLnBu885Wd1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_gradient_descent(0.01)"
      ],
      "metadata": {
        "id": "o2M_2ndx-ojD"
      },
      "id": "o2M_2ndx-ojD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_gradient_descent(0.001)"
      ],
      "metadata": {
        "id": "aaB-6yd05c0b"
      },
      "id": "aaB-6yd05c0b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_gradient_descent(0.001)"
      ],
      "metadata": {
        "id": "sNB8gYUu5fVF"
      },
      "id": "sNB8gYUu5fVF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate=0.0001\n",
        "iterations=500\n",
        "#gradient descent without regularisation\n",
        "theta,cost_history = batch_gradient_descent(X_train,y_train,theta,learning_rate,iterations)"
      ],
      "metadata": {
        "id": "GR6ui2265jrC"
      },
      "id": "GR6ui2265jrC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Cost Function vs Iterations, Weights = normal, LR = 0.0001 , itr = 1500')\n",
        "plt.plot(list(range(iterations)), cost_history, '-r') #plot the cost function.\n"
      ],
      "metadata": {
        "id": "QziWax-E5nGh"
      },
      "id": "QziWax-E5nGh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If you have low learning rate means your cost function will decrease slowly but in case of large learning rate cost function will decrease very fast.**"
      ],
      "metadata": {
        "id": "wGkWnfIXUR0c"
      },
      "id": "wGkWnfIXUR0c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Mini-batch Gradient Descent**"
      ],
      "metadata": {
        "id": "Wt-2Fhf4WkQf"
      },
      "id": "Wt-2Fhf4WkQf"
    },
    {
      "cell_type": "code",
      "source": [
        "def mini_batch_gradient_descent(X, y, theta, learning_rate, iterations, batch_size=32):\n",
        "    cost_history = np.zeros(iterations)\n",
        "    m = len(y)\n",
        "    \n",
        "    for i in range(iterations):\n",
        "        # Shuffle dataset\n",
        "        permutation = np.random.permutation(m)\n",
        "        X_shuffle = X[permutation,:]\n",
        "        y_shuffle = y[permutation]\n",
        "        \n",
        "        # Loop through each batch in the shuffled dataset\n",
        "        for j in range(0, m, batch_size):\n",
        "            X_batch = X_shuffle[j:j+batch_size,:]\n",
        "            y_batch = y_shuffle[j:j+batch_size].reshape(-1, 1)\n",
        "            \n",
        "            # Calculate gradient using batch\n",
        "            gradient = (1/batch_size)* np.dot(X_batch.transpose(), (hypothesis(X_batch,theta)- y_batch))\n",
        "            \n",
        "            # Update theta using the gradient and learning rate\n",
        "            theta = theta - learning_rate*gradient\n",
        "        \n",
        "        # Calculate cost and store in history\n",
        "        cost_history[i] = cost(X, y, theta)\n",
        "        \n",
        "    return theta, cost_history\n",
        "def check_and_plot_mini_batch(learning_rate=0.01, iterations=1500):\n",
        "  iterations=1500\n",
        "  theta,cost_history = mini_batch_gradient_descent(X_train,y_train,theta,learning_rate,iterations)\n",
        "  prediction = predict(X, theta, threshold=0.5)\n",
        "  print(prediction)\n",
        "  plt.title('Learning curve (Cost Function vs Iterations), Weights = normal, LR = 0.01 , itr = 1500')\n",
        "  plt.plot(list(range(iterations)), cost_history, '-r') #plot the cost function.\n"
      ],
      "metadata": {
        "id": "w01G0QYbD0tL"
      },
      "id": "w01G0QYbD0tL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_and_plot_mini_batch(learning_rate=0.01, iterations=1500)"
      ],
      "metadata": {
        "id": "LiyvEHLREQlU"
      },
      "id": "LiyvEHLREQlU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_and_plot_mini_batch(learning_rate=0.001, iterations=1500)"
      ],
      "metadata": {
        "id": "jMHR9b9bElKv"
      },
      "id": "jMHR9b9bElKv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_and_plot_mini_batch(learning_rate=0.0001, iterations=1500)"
      ],
      "metadata": {
        "id": "8rICUJDQEsYr"
      },
      "id": "8rICUJDQEsYr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Stochastic Gradient Descent**"
      ],
      "metadata": {
        "id": "JB2cxcOYWrs_"
      },
      "id": "JB2cxcOYWrs_"
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_gradient_descent(X, y, theta, learning_rate, iterations):\n",
        "    cost_history = np.zeros(iterations)\n",
        "    m = len(y)\n",
        "    \n",
        "    for i in range(iterations):\n",
        "        # Loop through each sample in the dataset\n",
        "        for j in range(m):\n",
        "            # Select a random sample from the dataset\n",
        "            random_index = np.random.randint(0, m)\n",
        "            x_i = X[random_index,:].reshape(1, X.shape[1])\n",
        "            y_i = y[random_index].reshape(1, 1)\n",
        "            \n",
        "            # Calculate gradient for the selected sample\n",
        "            gradient = np.dot(x_i.transpose(), (hypothesis(x_i,theta) - y_i))\n",
        "            \n",
        "            # Update theta using the gradient and learning rate\n",
        "            theta = theta - learning_rate*gradient\n",
        "            \n",
        "        # Calculate cost and store in history\n",
        "        cost_history[i] = cost(X, y, theta)\n",
        "        \n",
        "    return theta, cost_history\n",
        "\n",
        "def check_and_plot_stochastic(learning_rate=0.01, iterations=1500):\n",
        "  iterations=1500\n",
        "  theta,cost_history = mini_batch_gradient_descent(X_train,y_train,theta,learning_rate,iterations)\n",
        "  prediction = predict(X, theta, threshold=0.5)\n",
        "  print(prediction)\n",
        "  plt.title('Learning curve (Cost Function vs Iterations), Weights = normal, LR = 0.01 , itr = 1500')\n",
        "  plt.plot(list(range(iterations)), cost_history, '-r') #plot the cost function."
      ],
      "metadata": {
        "id": "ycllCVIXD5Gv"
      },
      "id": "ycllCVIXD5Gv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_and_plot_stochastic(0.01, 1500)"
      ],
      "metadata": {
        "id": "EShyKnWgdnaX"
      },
      "id": "EShyKnWgdnaX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_and_plot_stochastic(0.001, 1500)"
      ],
      "metadata": {
        "id": "I5VV7Hj_dpUj"
      },
      "id": "I5VV7Hj_dpUj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_and_plot_stochastic(0.000 1, 1500)"
      ],
      "metadata": {
        "id": "fpw2abAfdqfm"
      },
      "id": "fpw2abAfdqfm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LR2**"
      ],
      "metadata": {
        "id": "VLB7ARFgKNQR"
      },
      "id": "VLB7ARFgKNQR"
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_norm.iloc[:, 2:].values\n",
        "y = np.where(df_norm.iloc[:, 1].values == 'M', 1, 0)\n",
        "\n",
        "# X = (X - np.mean(X, axis=0))/np.std(X,axis=0)\n",
        "\n",
        "ones=np.ones((X.shape[0],1))\n",
        "X=np.concatenate((ones,X),axis=1) \n",
        "\n",
        "theta=np.random.randn(X.shape[1],1)\n",
        "\n",
        "np.random.seed(1)\n",
        "indices = np.arange(len(X))\n",
        "np.random.shuffle(indices)\n",
        "n_train = int(len(X) * 2/3)\n",
        "train_indices, test_indices = indices[:n_train], indices[n_train:]\n",
        "X_train, y_train = X[train_indices], y[train_indices]\n",
        "X_test, y_test = X[test_indices], y[test_indices]\n"
      ],
      "metadata": {
        "id": "KsV7ytNghXpx"
      },
      "id": "KsV7ytNghXpx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predicted = predict(X_test,theta) #we put inside it the respective threshold needed\n",
        "print(\"accuracy:\" , accuracy(y_test , y_predicted))\n",
        "print(\"precision:\",precision(y_test , y_predicted))\n",
        "print(\"recall:\",recall(y_test , y_predicted))\n",
        "print(\"f score:\",f_score(y_test , y_predicted))"
      ],
      "metadata": {
        "id": "fkW178DgiExg"
      },
      "id": "fkW178DgiExg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Logistic Regression Modified"
      ],
      "metadata": {
        "id": "ddbTKxZ7_M02"
      },
      "id": "ddbTKxZ7_M02"
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class Logistic_Regression:\n",
        "\n",
        "  def __init__(self, learning_rate = 0.001, epochs = 1000):\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "  \n",
        "  #function to predict the class of a test data point\n",
        "  def predict(self, X, theta, threshold=0.5):\n",
        "      return (self.hypothesis(self.X, theta) >= threshold).astype(int)\n",
        "\n",
        "  def accuracy(self, actual,predicted):\n",
        "    return np.mean((actual == predicted).astype(int))\n",
        "\n",
        "  def recall(self, actual, predicted):\n",
        "    TP = np.count_nonzero(np.logical_and((actual == 1) , (predicted ==1)).astype(int))\n",
        "    FN = np.count_nonzero(np.logical_and((actual == 1) , (predicted ==0)).astype(int))\n",
        "    return TP/(TP + FN)\n",
        "\n",
        "  def f_score(self, actual , predicted):\n",
        "    P = precision(actual, predicted)\n",
        "    R = recall(actual, predicted)\n",
        "    return 2*P*R/(P + R)\n",
        "\n",
        "    #function to calculate precision of the classifier\n",
        "  def precision(self, actual, predicted):\n",
        "      TP = np.count_nonzero(np.logical_and((actual == 1) , (predicted ==1)).astype(int))\n",
        "      FP = np.count_nonzero(np.logical_and((actual == 0) , (predicted ==1)).astype(int))\n",
        "      return TP/(TP + FP)\n",
        "  #function to calculate sigmoid\n",
        "  def sigmoid(self, x):\n",
        "      return 1/(1 + np.exp(-x) )\n",
        "    #hypothesis for logistic regression\n",
        "  def hypothesis(self, X,theta):\n",
        "      return sigmoid(np.dot(X, theta))\n",
        "    #binary cross entropy cost function, i.e derivative which we have to minimize\n",
        "  def cost(self, X,y,theta):\n",
        "      h = hypothesis(X,theta)\n",
        "      return (-1/len(y))*(np.sum((y * np.log(h)) + (1-y)*np.log(1-h)))\n",
        "  def gradient_descent(self, X,y,theta,learning_rate,iterations):\n",
        "    cost_history=np.zeros(iterations)\n",
        "    m=len(y)\n",
        "    for i in range(iterations):\n",
        "        gradient = (1/m)* np.dot(X.transpose(), (hypothesis(X,theta)- y))\n",
        "        #w=w-ita*gradient\n",
        "        theta = theta - learning_rate*gradient\n",
        "        cost_history[i] = cost(X, y, theta)\n",
        "        \n",
        "    return theta,cost_history\n",
        "\n"
      ],
      "metadata": {
        "id": "cGrNaPbR_JP6"
      },
      "id": "cGrNaPbR_JP6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A6juGDJo_nbw"
      },
      "execution_count": null,
      "outputs": [],
      "id": "A6juGDJo_nbw"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b8xsxdox_nbw"
      },
      "execution_count": null,
      "outputs": [],
      "id": "b8xsxdox_nbw"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X2Kta-ZZ_nbx"
      },
      "execution_count": null,
      "outputs": [],
      "id": "X2Kta-ZZ_nbx"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ym1C_G2_nbx"
      },
      "execution_count": null,
      "outputs": [],
      "id": "2ym1C_G2_nbx"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zDNwDLoH_nby"
      },
      "execution_count": null,
      "outputs": [],
      "id": "zDNwDLoH_nby"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "skUq8ZhR_nby"
      },
      "execution_count": null,
      "outputs": [],
      "id": "skUq8ZhR_nby"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3NqUbii1_nbz"
      },
      "execution_count": null,
      "outputs": [],
      "id": "3NqUbii1_nbz"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_2IU5v0F_nbz"
      },
      "execution_count": null,
      "outputs": [],
      "id": "_2IU5v0F_nbz"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Opd1YiDs_nbz"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Opd1YiDs_nbz"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}